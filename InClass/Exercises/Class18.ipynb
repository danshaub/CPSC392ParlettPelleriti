{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Together\n",
    "\n",
    "K-means is one of the most simple clustering algorithms out there. You randomly (or, in order to make convergence quicker, cleverly) choose K centroids in the feature space, you assign each data point to the centroid/cluster closest to it, and then recalculate the centroid by taking the mean (for each predictor) of all the data points in each cluster. \n",
    "\n",
    "This process iteratively repeats until either 1) cluster assignments don't change from step to step OR 2) the centroid doesn't change much from step to step.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1RHRfcPIjIZ_-IMOE00gyzVadlaGxXPh8\" width=350px />\n",
    "\n",
    "One thing to keep in mind with K-Means is that is assumes *spherical* variance within each cluster. That means that K-means behaves as if--within each cluster--all predictors have the same variance. Because of this, it is often good to either check this assumption, or z-score your variables so that they're on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-Means Function\n",
    "\n",
    "Let's write our own simplified K-means function. Your function, `KM()` should take in two arguments:\n",
    "\n",
    "- `df` a dataframe with all of your data.\n",
    "- `k` the number of clusters to fit.\n",
    "\n",
    "and apply K-Means to it. Remember that the steps of K means are:\n",
    "\n",
    "**1**. Randomly select k centroids.\n",
    "- I recommend choosing `k` random data points from `df`. You can do this by using `np.random.choice(range(0,df.shape[0]), k)` to select the indices for `k` randomly selected rows. THEN use those indices to grab the chosen rows from df and store them.\n",
    "    \n",
    "**2**. Assign each data point from `df` to the closest centroid.\n",
    "- You'll need to calculate the distance between each data point and each centroid. Perhaps look at the KNN classwork to see how to do that using `np.linalg.norm()` (see Hint 3).\n",
    "    - I recommend storing cluster/centroid membership by having a dictionary with one key for each cluster/centroid, and the value is a list of row indices pertaining to the data points in each cluster (see HINT 1 for an example of this.\n",
    "    \n",
    "**3**. Re-calculate the cluster mean/centroid\n",
    "- For each centroid/cluster, find the mean value for each predictor/feature by taking the mean for that feature from all the data points assigned to the centroid/cluster.(see Hint 2)\n",
    "    \n",
    "**4**. Repeat Steps 2-3 until the change in centroid positions are all less than 0.0001\n",
    "- in other words, calculate how far each centroid moved. If all of them moved less than 0.0001 units, then stop.\n",
    "    \n",
    "**5**. Return the cluster assignments by returning the dictonary of the clusters and their memberships that you create in #2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT 1:\n",
    "\n",
    "Store your cluster memberships like this (in this case k = 3, and there are only 20 datapoints, but your function should take any k, and any number of data points):\n",
    "\n",
    "```\n",
    "clust = {\"0\": [0,7,4,5,12,18,20],\n",
    "          \"1\": [10,8,3,2,14,17,19],\n",
    "          \"2\": [1,6,7,9,11,13,15,16]}\n",
    "```\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT 2:\n",
    "\n",
    "If a cluster contained the following data points:\n",
    "\n",
    "|           | X1 | X2 | X3 |\n",
    "|-----------|----|----|----|\n",
    "| Person 1  | 5  | 2  | 9  |\n",
    "| Person 2  | 2  | 3  | 2  |\n",
    "| Person 3  | 1  | 6  | 1  |\n",
    "| Person 4  | 7  | 1  | 4  |\n",
    "| Person 5  | 3  | 2  | 5  |\n",
    "| Person 6  | 1  | 1  | 8  |\n",
    "| Person 7  | 7  | 0  | 6  |\n",
    "| Person 8  | 0  | 7  | 2  |\n",
    "| Person 9  | 2  | 3  | 7  |\n",
    "| Person 10 | 4  | 6  | 1  |\n",
    "\n",
    "\n",
    "Then the centroid for that cluster would be [a,b,c] where a, b, and c are the means of each column X1, X2, and X3:\n",
    "\n",
    "a = (5 + 2 + 1 + 7 + 3 + 1 + 7 + 0 + 2 + 4)/10 \n",
    "\n",
    "b = (2 + 3 + 6 + 1 + 2 + 1 + 0 + 7 + 3 + 6)/10\n",
    "\n",
    "c = (9 + 2 + 1 + 4 + 5 + 8 + 6 + 2 + 7 + 1)/10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HINT 3:\n",
    "\n",
    "To calculate the distance between two vectors, you can use:\n",
    "\n",
    "```\n",
    "distance_ab = np.linalg.norm(a-b)\n",
    "\n",
    "```\n",
    "\n",
    "where `a` and `b` are the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KM(df,k):\n",
    "    \n",
    "    # 1. randomly select k centroids\n",
    "    \n",
    "    ### YOUR CODE HERE ###-----------------------------------\n",
    "    \n",
    "    centroids = []\n",
    "    \n",
    "    for i in range(0,k):\n",
    "        centroids.append(df.iloc[np.random.choice(range(i,df.shape[0], k))])\n",
    "    \n",
    "    \n",
    "    print(centroids)\n",
    "    \n",
    "    ### /YOUR CODE HERE ###----------------------------------\n",
    "    \n",
    "    \n",
    "    # 2. iterate through all the rows in df, and assign them to the\n",
    "    # centroid closest to them, store those assignments in a dict\n",
    "    \n",
    "    closest = []\n",
    "    \n",
    "#     for i in range(0, df.shape[0]):\n",
    "#         distance = sys.maxsize\n",
    "#         center = 0\n",
    "#         indx = 0\n",
    "#         dfI = df.iloc[i]\n",
    "#         for j in centroids:\n",
    "#             distance_ij = abs(np.linalg.norm(dfI-j))\n",
    "#             if distance_ij < distance:\n",
    "#                 center = indx\n",
    "#                 distance = distance_ij\n",
    "#             indx = indx + 1\n",
    "            \n",
    "#         closest.append(center)\n",
    "    \n",
    "#     print(closest)\n",
    "\n",
    "    converged = False # has the algorithm converged yet?\n",
    "    \n",
    "    while not converged: # until the centroids stop moving\n",
    "        \n",
    "        # add all the dictionary keys for each cluster\n",
    "        clust = {}\n",
    "        for c in range(0,k):\n",
    "            clust[str(c)] = []\n",
    "            \n",
    "\n",
    "        for dataPoint in range(0, df.shape[0]):\n",
    "\n",
    "            # calculate distances between dataPoint and each centroid\n",
    "            \n",
    "            ### YOUR CODE HERE ###-----------------------------------\n",
    "            \n",
    "            dfI = df.iloc[dataPoint]\n",
    "            distances = []\n",
    "            \n",
    "            for c in range(0,k):\n",
    "                distances.append(abs(np.linalg.norm(dfI-c)))\n",
    "    \n",
    "            ### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "            # find the centroid that's closest\n",
    "            # hint: use np.argmin()\n",
    "            \n",
    "            clust[str(np.argmin(distances))].append(dataPoint)\n",
    "            \n",
    "            ### YOUR CODE HERE ###-----------------------------------\n",
    "            converged = True\n",
    "    \n",
    "            ### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "            # add dataPoint to that cluster in the clust dictionary\n",
    "            \n",
    "            ### YOUR CODE HERE ###-----------------------------------\n",
    "    \n",
    "    \n",
    "            ### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "        # 3. re-calculate the center/centroid of each cluster\n",
    "\n",
    "        #new_centroids = [[] for c in range(0,k)] #create an empty list of k lists to fill in later\n",
    "\n",
    "        #for c in range(0,k): \n",
    "            # for each cluster calculate the new centroid values\n",
    "            # hint: remember the new centroid value is the mean\n",
    "            # (for each predicor) of data points in that cluster\n",
    "\n",
    "            ### YOUR CODE HERE ###-----------------------------------\n",
    "            # calculate the new centroids and add them to new_centroids\n",
    "            \n",
    "    \n",
    "            ### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "        #new_centroids = np.array(new_centroids) # change new centroids to be an array so that #4 works\n",
    "\n",
    "        # 4. check whether you can stop iterating by checking whether the\n",
    "        # distance between the previous position and current position is\n",
    "        # less than 0.0001 for all k centroids.\n",
    "\n",
    "        # calculate the distance between the old centroid values, and new_centroids values\n",
    "        #change = np.array([np.linalg.norm(centroids[i]-new_centroids[i]) for i in range(0,k)])\n",
    "        \n",
    "        # check whether all of them moved less than 0.0001 units.\n",
    "        #converged = np.all(change < 0.0001)\n",
    "\n",
    "        # set new_centroids to be established centroids\n",
    "        #centroids = new_centroids\n",
    "\n",
    "\n",
    "    # 5. Return cluster memberships dictionary\n",
    "    print(clust)\n",
    "    return(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[py    31.863162\n",
      "r     20.673834\n",
      "Name: 184, dtype: float64, py    53.015136\n",
      "r     16.216788\n",
      "Name: 205, dtype: float64, py    56.570726\n",
      "r     33.215489\n",
      "Name: 210, dtype: float64, py    88.022476\n",
      "r     80.529736\n",
      "Name: 87, dtype: float64]\n",
      "{'0': [], '1': [], '2': [], '3': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/programmers3.csv\")\n",
    "\n",
    "data.head()\n",
    "\n",
    "# if you want to you can test your function on data!\n",
    "\n",
    "KM(data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using your K-Means Function\n",
    "\n",
    "Now that you have done the incredibly impressive work of writing (with some help!) your own K-means function. Let's use it and compare the results to what we'd get from `sklearn`!\n",
    "\n",
    "First, use your OWN function `KM()` to do K-means on `data` with k = 5. Then generate the cluster assingments using the code provided. Then make a ggplot scatterplot of your clusters.\n",
    "\n",
    "Second, use sklearn's `KMeans()` function to do K-means on `data` with k = 5. Then generate the cluster assignments using `.predict()`. Then make a ggplot scatterplot of your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING YOUR FUNCTION-----\n",
    "\n",
    "# run k-means using YOUR function\n",
    "\n",
    "### YOUR CODE HERE ###-----------------------------------\n",
    "clusters = ###\n",
    "### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "# generate assignments\n",
    "assignments = np.array([999 for row in range(0, data.shape[0])])\n",
    "\n",
    "for cluster in clusters:\n",
    "    assignments[clusters[cluster]] = cluster\n",
    "    \n",
    "data[\"assignments_ME\"] = assignments\n",
    "\n",
    "# create ggplot scatter plot of data, using x, y and color = \"assignments_ME\"\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING SKLEARN\n",
    "\n",
    "# create kmeans model\n",
    "\n",
    "### YOUR CODE HERE ###-----------------------------------\n",
    "\n",
    "### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "# fit kmeans model\n",
    "\n",
    "### YOUR CODE HERE ###-----------------------------------\n",
    "\n",
    "### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "# get assignments\n",
    "\n",
    "### YOUR CODE HERE ###-----------------------------------\n",
    "\n",
    "assignments_SK = ###\n",
    "\n",
    "### /YOUR CODE HERE ###----------------------------------\n",
    "\n",
    "\n",
    "# add assignments to data\n",
    "data[\"assignments_SK\"] = assignments_SK\n",
    "\n",
    "# create another ggplot scatter plot of data, using x, y and color = \"assignments_SK\"\n",
    "### YOUR CODE HERE ###-----------------------------------\n",
    "\n",
    "### /YOUR CODE HERE ###----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Question*\n",
    "\n",
    "How do your results compare?\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
