{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n",
    "from sklearn.model_selection import cross_val_score # cross validation metrics\n",
    "from sklearn.model_selection import cross_val_predict # cross validation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%precision %.7g\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# 0. Together\n",
    "\n",
    "K-Nearest Neighbors is a straightforward algorithm: given a training set, classify a new (unknown) data point by counting the K nearest known points, and choosing the most common classification.\n",
    "\n",
    "In this classwork we'll use ggplot to plot the boundaries of knn, and see how the size, shape, and overlap of clusters affect these boundries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKNN2D(Xdf,y,k):\n",
    "    # X can only have 2 dimensions becuase of plotting\n",
    "    Xdf.columns = [\"x0\", \"x1\"]\n",
    "\n",
    "    #grab the range of features for each feature\n",
    "    x0_range = np.linspace(min(Xdf[\"x0\"]) - np.std(Xdf[\"x0\"]),\n",
    "                           max(Xdf[\"x0\"]) + np.std(Xdf[\"x0\"]), num = 100)\n",
    "    x1_range = np.linspace(min(Xdf[\"x1\"]) - np.std(Xdf[\"x1\"]),\n",
    "                           max(Xdf[\"x1\"]) + np.std(Xdf[\"x1\"]), num = 100)\n",
    "\n",
    "    #get all possible points on graph\n",
    "    x0 = np.repeat(x0_range,100)\n",
    "    x1 = np.tile(x1_range,100)\n",
    "    x_grid = pd.DataFrame({\"x0\": x0, \"x1\": x1})\n",
    "\n",
    "    #build model\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(Xdf,y)\n",
    "\n",
    "    # bredict all background points\n",
    "    p = knn.predict(x_grid)\n",
    "    x_grid[\"p\"] = p #add to dataframe\n",
    "    \n",
    "    #build the plot\n",
    "    bound = (ggplot(x_grid, aes(x = \"x0\", y = \"x1\", color = \"factor(p)\")) +\n",
    "                 geom_point(alpha = 0.2, size = 0.2) + theme_minimal() +\n",
    "                 scale_color_discrete(name = \"Class\") +\n",
    "                 geom_point(data = Xdf, mapping = aes(x = \"x0\", y = \"x1\", color = \"factor(y)\"), size = 2))\n",
    "    print(bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generating Fake Clusters\n",
    "\n",
    "## 1.1 Explore\n",
    "\n",
    "We can use the sklearn function `make_blobs()` in order to generate fake groups of data. The `centers` variable stores the xy coordinates of the centers of each of our groups as tuples. The `cluster_std` variables stores the standard deviation for each cluster. `n` is the number of data points to be generated from the clusters.\n",
    "\n",
    "Changing `centers` changes how far apart the centers of the clusters are, whereas changing `cluster_std` changes how diffuse/spread out the clusters are. Try playing around with these numbers and using ggplot to plot the data (color by `factor(y)`). Notice how changing the numbers changes the layout of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### variables #################################################\n",
    "centers = [(-5, -5),\n",
    "           (5, 5)]\n",
    "cluster_std = [1, 1]\n",
    "n = 200\n",
    "### /variables #################################################\n",
    "\n",
    "X, y = make_blobs(n_samples=n, cluster_std=cluster_std,\n",
    "                  centers=centers, n_features=2, random_state=1)\n",
    "\n",
    "# make it into a dataframe for ggplot\n",
    "train = pd.DataFrame(X)\n",
    "train.columns = [\"x0\", \"x1\"]\n",
    "train[\"y\"] = y\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "# make a scatterplot of the data we just created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 How does changing k affect the decision boundary?\n",
    "\n",
    "Using the `plotKNN2d()` function, and the `X` and `y` data generated below, examine what happens to the decision boundaries as you try different k's (try 1,3,5,10, 25, and 50).\n",
    "\n",
    "How does changing k affect the decision boundary?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [(-2, -2),\n",
    "           (5, 5)]\n",
    "cluster_std = [7, 7]\n",
    "n = 200\n",
    "\n",
    "X, y = make_blobs(n_samples=n, cluster_std=cluster_std,\n",
    "                  centers=centers, n_features=2, random_state=1)\n",
    "\n",
    "Xdf = pd.DataFrame(X)\n",
    "\n",
    "### MAKE A SCATTER PLOT OF THE DATA ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 How does changing k affect the decision boundary (highly separable groups)?\n",
    "\n",
    "Now let's see how changing k affects the boundary when the groups are not overlapping much. Using the `plotKNN2d()` function, and the `X` and `y` data generated below, examine what happens to the decision boundaries as you try different k's (try 1,3,5,10, 25, and 50).\n",
    "\n",
    "How does changing k affect the decision boundary when the groups are FAR apart to begin with?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [(-5, -5),\n",
    "           (5, 5)]\n",
    "cluster_std = [2, 2]\n",
    "n = 200\n",
    "\n",
    "X, y = make_blobs(n_samples=n, cluster_std=cluster_std,\n",
    "                  centers=centers, n_features=2, random_state=1)\n",
    "\n",
    "Xdf = pd.DataFrame(X)\n",
    "\n",
    "### MAKE A SCATTER PLOT OF THE DATA ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 How does changing k affect the decision boundary (imbalanced classes)?\n",
    "\n",
    "Now let's see how changing k affects the boundary when the groups have different numbers of samples. Using the `plotKNN2d()` function, and the `X` and `y` data generated below, examine what happens to the decision boundaries as you try different k's (try 1,3,5,10, 25, 50, and **100**).\n",
    "\n",
    "How does changing k affect the decision boundary when the groups are imbalanced?\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [(-2, -2),\n",
    "           (5, 5)]\n",
    "cluster_std = [6, 6]\n",
    "\n",
    "n1 = 50\n",
    "n2 = 150\n",
    "\n",
    "X, y = make_blobs(n_samples=[n1,n2], cluster_std=cluster_std,\n",
    "                  centers=centers, n_features=2, random_state=1)\n",
    "\n",
    "Xdf = pd.DataFrame(X)\n",
    "\n",
    "### MAKE A SCATTER PLOT OF THE DATA ### \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The KNN Algorithm\n",
    "\n",
    "Now that you've explored the impact of k in different situations, let's build our own function, `myKNN()` that will calculate the predicted class of a datapoint `d` using the \"training data\" `X` and true classifications `y`. Your function should take in four arguments: an array for the data point we want to predict (`d`), a data frame with the training data (`X`), an array/series with the true classes for `X` (`y`), and `k` an integer for KNN.\n",
    "\n",
    "It should return the predicted class for `d`. Your function should be able to handle any number of predictors, and any number of rows in the training data. But you can assume that there are only TWO classes: `0` and `1`. Feel free to write helper functions if needed, but do not use pre-built sklearn functions to do this.\n",
    "\n",
    "(Note: notice that you don't really need to *train* this algorithm, most of the computation has to do with prediction rather than training).\n",
    "\n",
    "HINTS: [distances](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html), [k-nearest](https://stackoverflow.com/questions/34226400/find-the-index-of-the-k-smallest-values-of-a-numpy-array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myKNN(d, X, y, k):\n",
    "    # d is an ARRAY with that data points predictor values like: np.array([0, 0.273, 0.23, 1])\n",
    "    # d and X can have ANY number of predictors.\n",
    "    # X is a dataframe with the \"training\" data\n",
    "    # y is an array/series with the true classes for the training set\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    # calculate distances---\n",
    "\n",
    "    # find k smallest distances---\n",
    "\n",
    "    # get votes from k neighbors---\n",
    "\n",
    "    # find most common class---\n",
    "    \n",
    "    return(predictedClass)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will the computational expense of prediction increase as the number of data points in the training set increase?\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" width = 200px />"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
